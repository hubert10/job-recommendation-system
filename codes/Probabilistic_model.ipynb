{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadine/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n",
      "0.18.1\n",
      "1.11.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import pandas\n",
    "import numpy\n",
    "print (matplotlib.__version__)\n",
    "print (pandas.__version__)\n",
    "print (numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199964, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '/home/nadine/Pictures/Memoir/data'\n",
    "\n",
    "data = pd.read_csv(os.path.join(DATA_DIR, 'data.csv'))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Number of users = 2500 | Total Number of jobs = 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = data.UserId.unique().shape[0]\n",
    "n_jobs = data.JobId.unique().shape[0]\n",
    "print (' Total Number of users = ' + str(n_users) + ' | Total Number of jobs = ' + str(n_jobs) )\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Create a base class with scaffolding for our 3 baselines.\n",
    "\n",
    "def split_title(title):\n",
    "    \"\"\"Change \"BaselineMethod\" to \"Baseline Method\".\"\"\"\n",
    "    words = []\n",
    "    tmp = [title[0]]\n",
    "    for c in title[1:]:\n",
    "        if c.isupper():\n",
    "            words.append(''.join(tmp))\n",
    "            tmp = [c]\n",
    "        else:\n",
    "            tmp.append(c)\n",
    "    words.append(''.join(tmp))\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "class Baseline(object):\n",
    "    \"\"\"Calculate baseline predictions.\"\"\"\n",
    "\n",
    "    def __init__(self, train_data):\n",
    "        \"\"\"Simple heuristic-based transductive learning to fill in missing\n",
    "        values in data matrix.\"\"\"\n",
    "        self.predict(train_data.copy())\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        raise NotImplementedError(\n",
    "            'baseline prediction not implemented for base class')\n",
    "\n",
    "    def rmse(self, test_data):\n",
    "        \"\"\"Calculate root mean squared error for predictions on test data.\"\"\"\n",
    "        return rmse(test_data, self.predicted)\n",
    "\n",
    "    def __str__(self):\n",
    "        return split_title(self.__class__.__name__)\n",
    "\n",
    "\n",
    "\n",
    "# Implement the 3 baselines.\n",
    "\n",
    "class UniformRandomBaseline(Baseline):\n",
    "    \"\"\"Fill missing values with uniform random values.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        masked_train = np.ma.masked_array(train_data, nan_mask)\n",
    "        pmin, pmax = masked_train.min(), masked_train.max()\n",
    "        N = nan_mask.sum()\n",
    "        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n",
    "        self.predicted = train_data\n",
    "\n",
    "\n",
    "class GlobalMeanBaseline(Baseline):\n",
    "    \"\"\"Fill in missing values using the global mean.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        train_data[nan_mask] = train_data[~nan_mask].mean()\n",
    "        self.predicted = train_data\n",
    "\n",
    "\n",
    "class MeanOfMeansBaseline(Baseline):\n",
    "    \"\"\"Fill in missing values using mean of user/item/global means.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        masked_train = np.ma.masked_array(train_data, nan_mask)\n",
    "        global_mean = masked_train.mean()\n",
    "        user_means = masked_train.mean(axis=1)\n",
    "        item_means = masked_train.mean(axis=0)\n",
    "        self.predicted = train_data.copy()\n",
    "        n, m = train_data.shape\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if np.ma.isMA(item_means[j]):\n",
    "                    self.predicted[i,j] = np.mean(\n",
    "                        (global_mean, user_means[i]))\n",
    "                else:\n",
    "                    self.predicted[i,j] = np.mean(\n",
    "                        (global_mean, user_means[i], item_means[j]))\n",
    "\n",
    "\n",
    "baseline_methods = OrderedDict()\n",
    "baseline_methods['ur'] = UniformRandomBaseline\n",
    "baseline_methods['gm'] = GlobalMeanBaseline\n",
    "baseline_methods['mom'] = MeanOfMeansBaseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pymc3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-27594f4615fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymc3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pymc3"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "# Enable on-the-fly graph computations, but ignore\n",
    "# absence of intermediate test values.\n",
    "theano.config.compute_test_value = 'ignore'\n",
    "\n",
    "# Set up logging.\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class PMF(object):\n",
    "    \"\"\"Probabilistic Matrix Factorization model using pymc3.\"\"\"\n",
    "\n",
    "    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "        :param np.ndarray train: The training data to use for learning the model.\n",
    "        :param int dim: Dimensionality of the model; number of latent factors.\n",
    "        :param int alpha: Fixed precision for the likelihood function.\n",
    "        :param float std: Amount of noise to use for model initialization.\n",
    "        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n",
    "            These bounds will simply be used to cap the estimates produced for R.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.std = np.sqrt(1.0 / alpha)\n",
    "        self.bounds = bounds\n",
    "        self.data = train.copy()\n",
    "        n, m = self.data.shape\n",
    "\n",
    "        # Perform mean value imputation\n",
    "        nan_mask = np.isnan(self.data)\n",
    "        self.data[nan_mask] = self.data[~nan_mask].mean()\n",
    "\n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = 1 / self.data.var(axis=1).mean()\n",
    "        self.alpha_v = 1 / self.data.var(axis=0).mean()\n",
    "\n",
    "        # Specify the model.\n",
    "        logging.info('building the PMF model')\n",
    "        with pm.Model() as pmf:\n",
    "            U = pm.MvNormal(\n",
    "                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n",
    "                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "            V = pm.MvNormal(\n",
    "                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n",
    "                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "            R = pm.Normal(\n",
    "                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n",
    "                observed=self.data)\n",
    "\n",
    "        logging.info('done building the PMF model')\n",
    "        self.model = pmf\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import ujson as json\n",
    "except ImportError:\n",
    "    import json\n",
    "\n",
    "\n",
    "# First define functions to save our MAP estimate after it is found.\n",
    "# We adapt these from `pymc3`'s `backends` module, where the original\n",
    "# code is used to save the traces from MCMC samples.\n",
    "def save_np_vars(vars, savedir):\n",
    "    \"\"\"Save a dictionary of numpy variables to `savedir`. We assume\n",
    "    the directory does not exist; an OSError will be raised if it does.\n",
    "    \"\"\"\n",
    "    logging.info('writing numpy vars to directory: %s' % savedir)\n",
    "    if not os.path.isdir(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    shapes = {}\n",
    "    for varname in vars:\n",
    "        data = vars[varname]\n",
    "        var_file = os.path.join(savedir, varname + '.txt')\n",
    "        np.savetxt(var_file, data.reshape(-1, data.size))\n",
    "        shapes[varname] = data.shape\n",
    "\n",
    "        ## Store shape information for reloading.\n",
    "        shape_file = os.path.join(savedir, 'shapes.json')\n",
    "        with open(shape_file, 'w') as sfh:\n",
    "            json.dump(shapes, sfh)\n",
    "        print shape_file\n",
    "\n",
    "\n",
    "def load_np_vars(savedir):\n",
    "    \"\"\"Load numpy variables saved with `save_np_vars`.\"\"\"\n",
    "    shape_file = os.path.join(savedir, 'shapes.json')\n",
    "    with open(shape_file, 'r') as sfh:\n",
    "        shapes = json.load(sfh)\n",
    "\n",
    "    vars = {}\n",
    "    for varname, shape in shapes.items():\n",
    "        var_file = os.path.join(savedir, varname + '.txt')\n",
    "        vars[varname] = np.loadtxt(var_file).reshape(shape)\n",
    "\n",
    "    return vars\n",
    "\n",
    "\n",
    "# Now define the MAP estimation infrastructure.\n",
    "def _map_dir(self):\n",
    "    basename = 'pmf-map-d%d' % self.dim\n",
    "    return os.path.join(DATA_DIR, basename)\n",
    "\n",
    "def _find_map(self):\n",
    "    \"\"\"Find mode of posterior using Powell optimization.\"\"\"\n",
    "    tstart = time.time()\n",
    "    with self.model:\n",
    "        logging.info('finding PMF MAP using Powell optimization...')\n",
    "        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell, disp=True)\n",
    "\n",
    "    elapsed = int(time.time() - tstart)\n",
    "    logging.info('found PMF MAP in %d seconds' % elapsed)\n",
    "\n",
    "    # This is going to take a good deal of time to find, so let's save it.\n",
    "    save_np_vars(self._map, self.map_dir)\n",
    "\n",
    "def _load_map(self):\n",
    "    self._map = load_np_vars(self.map_dir)\n",
    "\n",
    "def _map(self):\n",
    "    try:\n",
    "        return self._map\n",
    "    except:\n",
    "        if os.path.isdir(self.map_dir):\n",
    "            self.load_map()\n",
    "        else:\n",
    "            self.find_map()\n",
    "        return self._map\n",
    "\n",
    "\n",
    "# Update our class with the new MAP infrastructure.\n",
    "PMF.find_map = _find_map\n",
    "PMF.load_map = _load_map\n",
    "PMF.map_dir = property(_map_dir)\n",
    "PMF.map = property(_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Draw MCMC samples.\n",
    "def _trace_dir(self):\n",
    "    basename = 'pmf-mcmc-d%d' % self.dim\n",
    "    return os.path.join(DATA_DIR, basename)\n",
    "\n",
    "def _draw_samples(self, nsamples=1000, njobs=2):\n",
    "    # First make sure the trace_dir does not already exist.\n",
    "    if os.path.isdir(self.trace_dir):\n",
    "        shutil.rmtree(self.trace_dir)\n",
    "\n",
    "    with self.model:\n",
    "        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n",
    "        backend = pm.backends.Text(self.trace_dir)\n",
    "        logging.info('backing up trace to directory: %s' % self.trace_dir)\n",
    "        self.trace = pm.sample(draws=nsamples, init='advi',\n",
    "                               n_init=150000, njobs=njobs, trace=backend)\n",
    "\n",
    "def _load_trace(self):\n",
    "    with self.model:\n",
    "        self.trace = pm.backends.text.load(self.trace_dir)\n",
    "\n",
    "\n",
    "# Update our class with the sampling infrastructure.\n",
    "PMF.trace_dir = property(_trace_dir)\n",
    "PMF.draw_samples = _draw_samples\n",
    "PMF.load_trace = _load_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _predict(self, U, V):\n",
    "    \"\"\"Estimate R from the given values of U and V.\"\"\"\n",
    "    R = np.dot(U, V.T)\n",
    "    n, m = R.shape\n",
    "    sample_R = np.array([\n",
    "        [np.random.normal(R[i,j], self.std) for j in range(m)]\n",
    "        for i in range(n)\n",
    "    ])\n",
    "\n",
    "    # bound ratings\n",
    "    low, high = self.bounds\n",
    "    sample_R[sample_R < low] = low\n",
    "    sample_R[sample_R > high] = high\n",
    "    return sample_R\n",
    "\n",
    "\n",
    "PMF.predict = _predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our evaluation function.\n",
    "def rmse(test_data, predicted):\n",
    "    \"\"\"Calculate root mean squared error.\n",
    "    Ignoring missing values in the test data.\n",
    "    \"\"\"\n",
    "    I = ~np.isnan(test_data)   # indicator for missing values\n",
    "    N = I.sum()                # number of non-missing values\n",
    "    sqerror = abs(test_data - predicted) ** 2  # squared error array\n",
    "    mse = sqerror[I].sum() / N                 # mean squared error\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "# Define a function for splitting train/test data.\n",
    "def split_train_test(data, percent_test=10):\n",
    "    \"\"\"Split the data into train/test sets.\n",
    "    :param int percent_test: Percentage of data to use for testing. Default 10.\n",
    "    \"\"\"\n",
    "    n, m = data.shape             # # users, # jokes\n",
    "    N = n * m                     # # cells in matrix\n",
    "    test_size = N / percent_test  # use 10% of data as test set\n",
    "    train_size = N - test_size    # and remainder for training\n",
    "\n",
    "    # Prepare train/test ndarrays.\n",
    "    train = data.copy().values\n",
    "    test = np.ones(data.shape) * np.nan\n",
    "\n",
    "    # Draw random sample of training data to use for testing.\n",
    "    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n",
    "    idx_pairs = zip(tosample[0], tosample[1])   # tuples of row/col index pairs\n",
    "    indices = np.arange(len(idx_pairs))         # indices of index pairs\n",
    "    sample = np.random.choice(indices, replace=False, size=test_size)\n",
    "\n",
    "    # Transfer random sample from train set to test set.\n",
    "    for idx in sample:\n",
    "        idx_pair = idx_pairs[idx]\n",
    "        test[idx_pair] = train[idx_pair]  # transfer to test set\n",
    "        train[idx_pair] = np.nan          # remove from train set\n",
    "\n",
    "    # Verify everything worked properly\n",
    "    assert(np.isnan(train).sum() == test_size)\n",
    "    assert(np.isnan(test).sum() == train_size)\n",
    "\n",
    "    # Finally, hash the indices and save the train/test sets.\n",
    "    index_string = ''.join(map(str, np.sort(sample)))\n",
    "    name = hashlib.sha1(index_string).hexdigest()\n",
    "    savedir = os.path.join(DATA_DIR, name)\n",
    "    save_np_vars({'train': train, 'test': test}, savedir)\n",
    "    print savedir\n",
    "    # Return train set, test set, and unique hash of indices.\n",
    "    return train, test, name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_test_set(data, num_test=3):\n",
    "    \"\"\"Extract a test set from the data and return it, modifying the original\n",
    "    data in place to remove the test data extracted.\n",
    "    :param DataFrame data: The data to split into test/train.\n",
    "    :param int num_test: Number of test ratings to sample for each user.\n",
    "    :return: test DataFrame, which has the same shape as `data`.\n",
    "    \"\"\"\n",
    "    # make sure index and columns are both ints\n",
    "    data.index = data.index.astype(np.int)\n",
    "    data.columns = data.columns.astype(np.int)\n",
    "\n",
    "    # randomly select 3 ratings for each user to use as test data\n",
    "    # for now, just assume we have 3 valid options\n",
    "    valid = data.notnull()\n",
    "    test = pd.DataFrame(np.zeros(data.shape))\n",
    "    test.columns = data.columns\n",
    "    test.index = data.index\n",
    "    for row in xrange(len(data)):\n",
    "        data_row = data.ix[row]\n",
    "        options = data_row[valid.ix[row]]\n",
    "        choices = np.random.choice(options.index, replace=False, size=num_test)\n",
    "        test.ix[row].ix[choices] = data_row.ix[choices]\n",
    "        data_row.ix[choices] = np.nan\n",
    "\n",
    "    return test\n",
    "\n",
    "\n",
    "def load_jester_data(fname='data/data.csv', nrows=1000):\n",
    "    data = pd.read_csv(fname, index_col=0, header=False, nrows=nrows)\n",
    "    data += 5  # ratings are from -10 to 10, lets make them 0 to 20\n",
    "    test = extract_test_set(data)\n",
    "    return data, test\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see the results:\n",
    "baselines = {}\n",
    "for name in baseline_methods:\n",
    "    Method = baseline_methods[name]\n",
    "    method = Method(train)\n",
    "    baselines[name] = method.rmse(test)\n",
    "    print('%s RMSE:\\t%.5f' % (method, baselines[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_train_test(name):\n",
    "    \"\"\"Load the train/test sets.\"\"\"\n",
    "    savedir = os.path.join(DATA_DIR, name)\n",
    "    vars = load_np_vars(savedir)\n",
    "    return vars['train'], vars['test']\n",
    "\n",
    "# train, test, name = split_train_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = load_train_test('6bb8d06c69c0666e6da14c094d4320d115f1ffc8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "hash_object = hashlib.sha1(b'Hello World')\n",
    "hex_dig = hash_object.hexdigest()\n",
    "print(hex_dig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
