{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n",
      "0.19.2\n",
      "1.11.2\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import pandas\n",
    "import numpy\n",
    "print (matplotlib.__version__)\n",
    "print (pandas.__version__)\n",
    "print (numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading ratings file:\n",
    "df = pd.read_csv('/home/nadine/Pictures/Memoir/data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Number of users = 2500 | Total Number of jobs = 150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.UserId.unique().shape[0]\n",
    "n_jobs = df.JobId.unique().shape[0]\n",
    "print (' Total Number of users = ' + str(n_users) + ' | Total Number of jobs = ' + str(n_jobs) )\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df,test_size=0.25)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "#Create two user-item matrices, one for training and another for testing\n",
    "train_data_matrix = np.zeros((n_users, n_jobs))\n",
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_jobs))\n",
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Memory-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #You use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get the size of rating's data\n",
    "#print (rating.shape)\n",
    "# To get the firts five users!\n",
    "#rating.head()\n",
    "#len(rating.UserId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_prediction = predict(train_data_matrix, item_similarity, type='item')\n",
    "user_prediction = predict(train_data_matrix, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  4.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data_matrix,test_data_matrix,\n",
    "train_data_matrix[:1,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Compute the cosine similarity\n",
    "## A distance metric commonly used in Recommender system is \n",
    "## cosine simmilarity and uses rating as a n-dimensional matrix\n",
    "## and the similarity is calculated based on the angle between these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.71749608  0.69088184  0.66317056]\n",
      " [ 0.71749608  0.          0.69267819  0.66449866]\n",
      " [ 0.69088184  0.69267819  0.          0.66424222]\n",
      " [ 0.66317056  0.66449866  0.66424222  0.        ]]\n",
      "[[ 0.          0.70296332  0.79714053  0.76765588]\n",
      " [ 0.70296332  0.          0.5437813   0.71207111]\n",
      " [ 0.79714053  0.5437813   0.          0.73031612]\n",
      " [ 0.76765588  0.71207111  0.73031612  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(item_similarity[:4, :4])\n",
    "print(user_similarity[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.702963</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.000000  0.702963\n",
       "1  0.702963  0.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sims = pd.DataFrame(user_similarity)\n",
    "data_sims.ix[:1, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_sims.ix[:,:1]\n",
    "#data_sims.ix[1]\n",
    "#data_sims[jobs_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 2.58677253492\n",
      "Item-based CF RMSE: 2.5875647934\n"
     ]
    }
   ],
   "source": [
    "print 'User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix))\n",
    "print 'Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_top_2(jobs_data):\n",
    "#    p = data_sims[jobs_data].apply(lambda row: np.sum(row), axis=1)\n",
    "#    p = p.order(ascending=False)\n",
    "#    return p.index[p.index.isin(jobs_data)==False][:2] \n",
    "#print get_top_2()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 509 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# Let us check the computational time\n",
    "%timeit pairwise_distances(train_data_matrix, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        #You use np.newaxis so that mean_user_rating has same format as ratings\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'job':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])     \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity level of MovieLens100K is 46.7%\n"
     ]
    }
   ],
   "source": [
    "sparsity=round(1.0-len(df)/float(n_users*n_jobs),3)\n",
    "print 'The sparsity level of MovieLens100K is ' +  str(sparsity*100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with the calculated simalarity matrix now we can use it to predict tyhe non rated jobs offers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 2.69042773512\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print 'User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.11727412,  1.0889341 ,  1.10473179, ...,  1.08762826,\n",
       "         1.08887162,  1.12181355],\n",
       "       [ 1.11518887,  1.10337874,  1.11654551, ...,  1.11662579,\n",
       "         1.1154989 ,  1.11956241],\n",
       "       [ 1.22084343,  1.22275831,  1.223639  , ...,  1.22181234,\n",
       "         1.19570289,  1.20923472],\n",
       "       ..., \n",
       "       [ 1.14988749,  1.18423508,  1.18071989, ...,  1.17841764,\n",
       "         1.14284915,  1.17999281],\n",
       "       [ 1.01123028,  0.99042381,  1.00432241, ...,  1.01582848,\n",
       "         1.01496722,  0.98103633],\n",
       "       [ 0.86856934,  0.86340961,  0.86815485, ...,  0.86729355,\n",
       "         0.84597833,  0.86687515]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 59.2 ms per loop\n",
      "1 loop, best of 3: 886 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit predict(train_data_matrix, item_similarity, type='job')\n",
    "%timeit predict(train_data_matrix, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF RMSE: 2.58677253492\n",
      "Item-based CF RMSE: 2.5875647934\n"
     ]
    }
   ],
   "source": [
    "print 'User-based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix))\n",
    "print 'Item-based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparsity level of WutikoData is 46.7%\n"
     ]
    }
   ],
   "source": [
    "sparsity=round(1.0-len(df)/float(n_users*n_jobs),3)\n",
    "print 'The sparsity level of WutikoData is ' +  str(sparsity*100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 2.69042773512\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print 'User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Content-Boosted Collaborative Filtering:\n",
    "# This approach uses a content-based predictor to enhance existing user data,\n",
    "# and then provides personalized suggestions through collaborative filtering\n",
    "# Content-Boosted Collaborative Filtering, performs better than a pure content-based predictor, pure\n",
    "# collaborative filter, and a naive hybrid approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# The metrics for evaluating the accuracy of a prediction algorithm can be divided into two main categories: \n",
    "# statistical accuracy metrics and decision-support metrics.\n",
    "# 1. Statistical accuracy metrics evaluate the accu-\n",
    "# racy of a predictor by comparing predicted values with user-\n",
    "#provided values. To measure statistical accuracy we use the\n",
    "# mean absolute error (MAE) metric — defined as the average\n",
    "# absolute difference between predicted ratings and actual rat-\n",
    "# ings. In our experiments we computed the MAE on the test\n",
    "# set for each user, and then averaged over the set of test users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2.Decision-support accuracy measures how well predic-\n",
    "# tions help users select high-quality items. We use Re-\n",
    "# ceiver Operating Characteristic (ROC) sensitivity to mea-\n",
    "# sure decision-support accuracy. A predictor can be treated as a filter, where predicting a high rating for an item is\n",
    "# equivalent to accepting the item, and predicting a low rating\n",
    "# is equivalent to rejecting the item. The ROC sensitivity is\n",
    "# given by the area under the ROC curve — a curve that plots\n",
    "# sensitivity versus 1-specificity for a predictor. Sensitivity is\n",
    "# defined as the probability that a good item is accepted by the\n",
    "# filter; and specificity is defined as the probability that a bad\n",
    "# item is rejected by the filter. We consider an item good if the\n",
    "# user gave it a rating of 4 or above, otherwise we consider the\n",
    "# item bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
